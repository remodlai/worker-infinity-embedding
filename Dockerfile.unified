# Unified Dockerfile for RunPod Infinity Worker with Qwen3 models
FROM nvidia/cuda:12.4.1-cudnn-runtime-ubuntu22.04

ENV DEBIAN_FRONTEND=noninteractive
ENV TZ=UTC
ENV HF_HOME=/runpod-volume

# Install Python 3.12
RUN apt-get update && apt-get install -y \
    software-properties-common \
    git \
    wget \
    && add-apt-repository ppa:deadsnakes/ppa \
    && apt-get update \
    && apt-get install -y python3.12 python3.12-dev python3.12-venv \
    && wget https://bootstrap.pypa.io/get-pip.py \
    && python3.12 get-pip.py \
    && ln -sf /usr/bin/python3.12 /usr/bin/python \
    && ln -sf /usr/local/bin/pip3.12 /usr/bin/pip \
    && rm get-pip.py \
    && apt-get clean && rm -rf /var/lib/apt/lists/*

# Install torch first with cuda index
RUN pip install --no-cache-dir torch==2.5.1 --index-url https://download.pytorch.org/whl/cu124

# Install infinity-emb from our fork with Qwen3 support (with all extras)
RUN pip install --no-cache-dir "git+https://github.com/remodlai/infinity-embeddings-qwen3support.git@main#subdirectory=libs/infinity_emb&egg=infinity-emb[torch,server]"

# Install additional dependencies with specific versions for compatibility
RUN pip install --no-cache-dir transformers==4.51.3 sentence-transformers accelerate

# Install uv for faster dependency installation
RUN pip install uv

# Install RunPod and other worker dependencies
COPY requirements.txt /requirements.txt
RUN uv pip install -r /requirements.txt --system

# Copy models into the container (these layers will be cached)
COPY models/hub/models--Qwen--Qwen3-Embedding-0.6B /models/Qwen3-Embedding-0.6B
COPY models/hub/models--Qwen--Qwen3-Reranker-0.6B /models/Qwen3-Reranker-0.6B

# Add worker source files
ADD src .

# Add test input
COPY test_input.json /test_input.json

# Expose the RunPod serverless port
EXPOSE 8000

# Use ENTRYPOINT and CMD as recommended by RunPod
ENTRYPOINT ["python", "-u"]
CMD ["/handler.py"]