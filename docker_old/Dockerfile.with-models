FROM nvidia/cuda:12.4.1-cudnn-runtime-ubuntu22.04 AS base

# Pre-download models during build
ENV HF_HOME=/models
ENV TRANSFORMERS_CACHE=/models/huggingface
ENV HF_DATASETS_CACHE=/models/huggingface/datasets

# install python and other packages
RUN apt-get update && apt-get install -y \
    python3.11 \
    python3-pip \
    git \
    wget \
    libgl1 \
    && ln -sf /usr/bin/python3.11 /usr/bin/python \
    && ln -sf /usr/bin/pip3 /usr/bin/pip

# install uv
RUN pip install uv

# install python dependencies
COPY requirements.txt /requirements.txt
RUN uv pip install -r /requirements.txt --system

# install torch
RUN pip install torch==2.5.1+cu124 --index-url https://download.pytorch.org/whl/test/cu124 --no-cache-dir

# Install sentence-transformers and optimum (needed for torch backend and BetterTransformer)
RUN pip install sentence-transformers optimum accelerate

# Pre-download the Qwen3 0.6B models
RUN python -c "from transformers import AutoModel, AutoTokenizer; \
    print('Downloading Qwen3-Embedding-0.6B...'); \
    AutoModel.from_pretrained('Qwen/Qwen3-Embedding-0.6B'); \
    AutoTokenizer.from_pretrained('Qwen/Qwen3-Embedding-0.6B'); \
    print('Downloading Qwen3-Reranker-0.6B...'); \
    AutoModel.from_pretrained('Qwen/Qwen3-Reranker-0.6B'); \
    AutoTokenizer.from_pretrained('Qwen/Qwen3-Reranker-0.6B'); \
    print('Models downloaded successfully!')"

# Set HF_HOME back to runpod-volume for runtime
ENV HF_HOME=/runpod-volume

# Add src files
ADD src .

# Add test input
COPY test_input.json /test_input.json

# Expose the RunPod serverless port
EXPOSE 8000

# Use ENTRYPOINT and CMD as recommended by RunPod
ENTRYPOINT ["python", "-u"]
CMD ["/handler.py"]